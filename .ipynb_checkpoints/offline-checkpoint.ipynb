{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_prefix(df,prefix):\n",
    "    '''\n",
    "    Filter case by prefix length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to slice by prefix length\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe with sliced cases\n",
    "    '''\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    for case,group in groups: \n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group)>prefix:\n",
    "            group = group.loc[:prefix-1,:]\n",
    "            encoded_df.append(group)\n",
    "    return pd.concat(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_encoding(df, prefix):\n",
    "    '''\n",
    "    Aggregation encoding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to encode for outcome prediction\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe encoded in aggregation method\n",
    "    '''\n",
    "    df = filter_by_prefix(df,prefix)\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    for case,group in groups: \n",
    "        group = group.reset_index(drop=True)\n",
    "        outcome = set(group['outcome']).pop()\n",
    "        cumdurationlist = [(x - list(group['ts'])[0]).total_seconds() for x in list(group['ts'])]\n",
    "        case_time_outcome = {'caseid':case, 'ts':np.mean(cumdurationlist),'outcome':outcome}\n",
    "        activity_count = {x: list(group['activity']).count(x) for x in set(group['activity'])}\n",
    "        resource_count = {x: list(group['resource']).count(x) for x in set(group['resource'])}\n",
    "\n",
    "        case_time_outcome.update(activity_count)\n",
    "        case_time_outcome.update(resource_count)\n",
    "        dfk = pd.DataFrame.from_dict([case_time_outcome])\n",
    "        encoded_df.append(dfk)\n",
    "    concated_df = pd.concat(encoded_df)\n",
    "    concated_df = concated_df.fillna(0)\n",
    "    return concated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexbase_encoding(df, prefix):\n",
    "    '''\n",
    "    Indexbase encoding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to encode for outcome prediction\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe encoded in indexbase method\n",
    "    '''\n",
    "    df = filter_by_prefix(df,prefix)\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    for case,group in groups: \n",
    "        activitylist = list(group['activity'])\n",
    "        resourcelist = list(group['resource'])\n",
    "        group = group.reset_index(drop=True)\n",
    "        outcome = set(group['outcome']).pop()\n",
    "        cumdurationlist = [(x - list(group['ts'])[0]).total_seconds() for x in list(group['ts'])]\n",
    "        cumduration_index ={'Cumduration_'+str(x+1): cumdurationlist[x] for x in range(len(cumdurationlist))}\n",
    "        case_outcome = {'caseid':case, 'outcome':outcome}\n",
    "        activity_index = {'activity_'+str(x+1)+'_'+activitylist[x]: 1 for x in range(len(activitylist))}\n",
    "        resource_index = {'resource_'+str(x+1)+'_'+str(resourcelist[x]): 1 for x in range(len(resourcelist))}\n",
    "        case_outcome.update(cumduration_index)\n",
    "        case_outcome.update(activity_index)\n",
    "        case_outcome.update(resource_index)\n",
    "        dfk = pd.DataFrame.from_dict([case_outcome])\n",
    "        encoded_df.append(dfk)\n",
    "    concated_df = pd.concat(encoded_df)\n",
    "    concated_df = concated_df.fillna(0)\n",
    "    return concated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/bac_online_back_small.csv')\n",
    "# df['START_DATE'] = pd.to_datetime(df['START_DATE'])\n",
    "# df = df.rename(columns={'REQUEST_ID':'caseid','ACTIVITY':'activity','START_DATE':'ts','CE_UO':'resource'})\n",
    "# df = df.loc[:,['caseid','activity','ts','resource','outcome']]\n",
    "\n",
    "# df = pd.read_csv('./data/IRO5k_labeled_sampled_newts.csv')\n",
    "# key_pair = {\n",
    "#         'Case ID':'caseid',\n",
    "#         'Activity':'activity',\n",
    "#         'Complete Timestamp':'ts',\n",
    "# }\n",
    "# df['Complete Timestamp'] = pd.to_datetime(df['Complete Timestamp'])\n",
    "# df = df.rename(columns=key_pair)\n",
    "# df = df.loc[:,['caseid','activity','ts','outcome']]\n",
    "\n",
    "# df = pd.read_csv('./data/bpic15_streaming.csv')\n",
    "# key_pair = {\n",
    "# }\n",
    "# df = df.rename(columns=key_pair)\n",
    "# df = df.loc[:,['caseid','activity','ts','resource','outcome']]\n",
    "\n",
    "\n",
    "# df = pd.read_csv('./data/BPI Challenge 2017_modified3.csv')\n",
    "# key_pair = {\n",
    "#     'Case ID':'caseid',\n",
    "#     'Activity':'activity',\n",
    "#     'Resource':'resource',\n",
    "#     'Start Timestamp':'ts',\n",
    "#     'Outcome':'outcome'\n",
    "# }\n",
    "# df = df.rename(columns=key_pair)\n",
    "# df = df.loc[:,['caseid','activity','ts','resource','outcome']]\n",
    "\n",
    "df = pd.read_csv('./data/road_traffic_fine_process.csv')\n",
    "key_pair = {\n",
    "    'Case ID':'caseid',\n",
    "    'Activity':'activity',\n",
    "    'Resource':'resource',\n",
    "    'Complete Timestamp':'ts',\n",
    "}\n",
    "df = df.rename(columns=key_pair)\n",
    "df = df.loc[:,['caseid','activity','ts','resource','outcome']]\n",
    "\n",
    "\n",
    "save_dir = 'road_traffic_fine_process'\n",
    "\n",
    "try:\n",
    "    os.makedirs('./result/%s'%(save_dir))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('caseid')\n",
    "concating = []\n",
    "for _, group in groups:\n",
    "    outcomelist = list(group['outcome'])\n",
    "    outcome = outcomelist[-1]\n",
    "    group = group.reset_index(drop=True)\n",
    "    if True in outcomelist:\n",
    "        group = group.loc[:outcomelist.index(True),:]\n",
    "    group['outcome'] = outcome\n",
    "    concating.append(group)\n",
    "\n",
    "dfn = pd.concat(concating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressing length: 2\n",
      "Progressing length: 3\n",
      "Progressing length: 4\n",
      "Progressing length: 5\n"
     ]
    }
   ],
   "source": [
    "idslist = []\n",
    "prefix_length=6\n",
    "for length in range(2,prefix_length):\n",
    "    print('Progressing length: %s'%(length))\n",
    "    idslist.append(indexbase_encoding(dfn,length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC aggregation encoding with prefix length 2\n",
      "BAC aggregation encoding with prefix length 3\n",
      "BAC aggregation encoding with prefix length 4\n",
      "BAC aggregation encoding with prefix length 5\n",
      "{'prefix_2': 0.5994557823129251, 'prefix_3': 0.7225600000000001, 'prefix_4': 0.8190016103059582, 'prefix_5': 0.7300813008130081}\n"
     ]
    }
   ],
   "source": [
    "prefixlist= list(range(2,prefix_length))\n",
    "acc_dict= {}\n",
    "for pos,prefix in enumerate(idslist):\n",
    "    print('BAC aggregation encoding with prefix length %s'%(prefixlist[pos]))\n",
    "    y = prefix['outcome']\n",
    "    x =prefix.drop(columns=['outcome','caseid'],axis=1)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3)\n",
    "\n",
    "        # Deicision tree result\n",
    "        dt = DecisionTreeClassifier(criterion='entropy').fit(x_train,y_train)\n",
    "        y_pred = dt.predict(x_test)\n",
    "        acc_list.append(accuracy_score(y_test,y_pred))\n",
    "    acc_dict['prefix_%s'%(str(prefixlist[pos]))] =  np.mean(acc_list)\n",
    "\n",
    "print(acc_dict)\n",
    "import pickle as pkl\n",
    "\n",
    "x = list(acc_dict.keys())\n",
    "y = [acc_dict[x] for x in acc_dict.keys()]\n",
    "with open('./result/%s/off_dt_acc.pkl'%(save_dir),'wb') as f:\n",
    "    pkl.dump([x,y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAC index-base encoding with prefix length 2\n",
      "BAC index-base encoding with prefix length 3\n",
      "BAC index-base encoding with prefix length 4\n",
      "BAC index-base encoding with prefix length 5\n",
      "{'prefix_2': 0.6300680272108843, 'prefix_3': 0.78784, 'prefix_4': 0.8679549114331724, 'prefix_5': 0.8203252032520325}\n"
     ]
    }
   ],
   "source": [
    "prefixlist= list(range(2,prefix_length))\n",
    "acc_dict= {}\n",
    "for pos,prefix in enumerate(idslist):\n",
    "    print('BAC index-base encoding with prefix length %s'%(prefixlist[pos]))\n",
    "    y = prefix['outcome']\n",
    "    x =prefix.drop(columns=['outcome','caseid'],axis=1)\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(10):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3)\n",
    "\n",
    "        # Deicision tree result\n",
    "        rf = RandomForestClassifier(criterion='entropy').fit(x_train,y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        acc_list.append(accuracy_score(y_test,y_pred))\n",
    "    acc_dict['prefix_%s'%(str(prefixlist[pos]))] =  np.mean(acc_list)\n",
    "print(acc_dict)\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "x = list(acc_dict.keys())\n",
    "y = [acc_dict[x] for x in acc_dict.keys()]\n",
    "with open('./result/%s/off_rf_acc.pkl'%(save_dir),'wb') as f:\n",
    "    pkl.dump([x,y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
