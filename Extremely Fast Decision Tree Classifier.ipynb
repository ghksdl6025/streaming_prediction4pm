{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream,tree,metrics\n",
    "import utils\n",
    "import datetime\n",
    "from encoding import prefix_bin\n",
    "import csv\n",
    "import copy\n",
    "import time\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stream.iter_csv(\n",
    "            './data/bac_online_small.csv',\n",
    "            )\n",
    "\n",
    "totallength = len(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stream.iter_csv(\n",
    "            './data/bac_online_small.csv',\n",
    "            drop=['END_DATE','ROLE','CLOSURE_TYPE','CLOSURE_REASON','WORKING_STATE','case_cost'],\n",
    "            target='outcome'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Event stream entry**  \n",
    "----------  \n",
    "**Example 1)**  \n",
    "({'REQUEST_ID': '20175000168',  \n",
    "  'ACTIVITY': 'Service closure Request with network responsibility',  \n",
    "  'START_DATE': '2018-10-10 12:48:12.000',  \n",
    "  'CE_UO': '1'},  \n",
    " '')  \n",
    "   \n",
    "**Example 2)**  \n",
    "({'REQUEST_ID': '20175000168',  \n",
    "  'ACTIVITY': 'Request completed with account closure',  \n",
    "  'START_DATE': '2018-10-17 03:03:11.000',  \n",
    "  'CE_UO': 'BOF'},  \n",
    " 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pair = {\n",
    "        'REQUEST_ID':'caseid',\n",
    "        'ACTIVITY':'activity',\n",
    "        'START_DATE':'ts',\n",
    "        'CE_UO':'resource'\n",
    "}\n",
    "\n",
    "case_dict ={}\n",
    "training_models ={}\n",
    "feature_matrix ={}\n",
    "casecount = 0\n",
    "rowcounter = 0\n",
    "resultdict ={}\n",
    "acc_dict ={}\n",
    "running_case = 0\n",
    "prediction_result = {}\n",
    "graceperiod_finish=0\n",
    "finishedcases = set()\n",
    "usedingrace = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % Case finished: 0\n",
      "0.76 % Case finished: 56\n",
      "1.52 % Case finished: 126\n",
      "2.28 % Case finished: 191\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset:\n",
    "    if rowcounter%500 == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount))\n",
    "    rowcounter +=1\n",
    "    # Event stream change dictionary keys\n",
    "    x = utils.dictkey_chg(x, key_pair)\n",
    "#     x['ts'] = x['ts'][:-4]\n",
    "    # Check label possible\n",
    "    # x = utils.set_label(x)\n",
    "    x['outcome'] =y \n",
    "    # Initialize case by prefix length\n",
    "    caseid = x['caseid']\n",
    "    outcome = x['outcome']\n",
    "#     progress = x['progress']\n",
    "\n",
    "    x.pop('caseid')\n",
    "    x.pop('outcome')\n",
    "    \n",
    "#     x.pop('progress')\n",
    "\n",
    "    case_bin = prefix_bin(caseid, x)\n",
    "\n",
    "    if caseid not in list(case_dict.keys()):\n",
    "        case_bin.set_prefix_length(1)    \n",
    "        case_dict[caseid] = []\n",
    "    elif caseid in finishedcases:\n",
    "        pass\n",
    "    else:\n",
    "        case_bin.set_prefix_length(len(case_dict[caseid])+1)\n",
    "        case_bin.set_prev_enc(case_dict[caseid][-1])\n",
    "    \n",
    "    # Encode event and cases and add to DB\n",
    "    case_bin.update_truelabel(outcome)   \n",
    "    case_bin.update_encoded()\n",
    "    ts = case_bin.event['ts']\n",
    "    case_dict[caseid].append(case_bin)\n",
    "    usedingrace.add(caseid)\n",
    "    # Detect label appeared case \n",
    "    if outcome != '' and caseid not in finishedcases:\n",
    "        finishedcases.add(caseid)\n",
    "        # Adding newly finished case to training set.    \n",
    "        casecount +=1\n",
    "        # Grace period to collect feature matrix\n",
    "        if casecount <200:\n",
    "            case_length = len(case_dict[caseid])\n",
    "            for prefix in range(1, case_length):\n",
    "                if 'prefix_%s'%(prefix+1) not in list(feature_matrix.keys()):\n",
    "                    feature_matrix['prefix_%s'%(prefix+1)]=set()\n",
    "                    # Initialize classifier and performance matrix and updating count\n",
    "                    training_models['prefix_%s'%(prefix+1)] = [tree.ExtremelyFastDecisionTreeClassifier(grace_period=100,split_criterion='info_gain'),metrics.Accuracy(), 0,0]\n",
    "                feature_list = list(case_dict[caseid][prefix].encoded.keys())\n",
    "                for x in feature_list: feature_matrix['prefix_%s'%(prefix+1)].add(x) \n",
    "            graceperiod_finish = case_dict[caseid][-1].event['ts']\n",
    "            for t in training_models.keys():\n",
    "                training_models[t][3] = graceperiod_finish\n",
    "            case_dict.pop(caseid)            \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513\n"
     ]
    }
   ],
   "source": [
    "print(len(usedingrace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_prefix = sorted([int(x.split('_')[1]) for x in training_models.keys()])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 % Case finished: 233 Running case: 84\n",
      "3.8 % Case finished: 284 Running case: 177\n",
      "4.57 % Case finished: 326 Running case: 250\n",
      "5.33 % Case finished: 359 Running case: 307\n",
      "6.09 % Case finished: 408 Running case: 354\n",
      "6.85 % Case finished: 449 Running case: 395\n",
      "7.61 % Case finished: 473 Running case: 426\n",
      "8.37 % Case finished: 498 Running case: 464\n",
      "9.13 % Case finished: 598 Running case: 412\n",
      "9.89 % Case finished: 696 Running case: 353\n",
      "10.65 % Case finished: 743 Running case: 372\n",
      "11.41 % Case finished: 798 Running case: 376\n",
      "12.17 % Case finished: 874 Running case: 365\n",
      "12.94 % Case finished: 955 Running case: 339\n",
      "13.7 % Case finished: 1042 Running case: 326\n",
      "14.46 % Case finished: 1106 Running case: 330\n",
      "15.22 % Case finished: 1173 Running case: 318\n",
      "15.98 % Case finished: 1254 Running case: 300\n",
      "16.74 % Case finished: 1332 Running case: 296\n",
      "17.5 % Case finished: 1398 Running case: 302\n",
      "18.26 % Case finished: 1446 Running case: 317\n",
      "19.02 % Case finished: 1503 Running case: 327\n",
      "19.78 % Case finished: 1606 Running case: 315\n",
      "20.54 % Case finished: 1671 Running case: 369\n",
      "21.31 % Case finished: 1737 Running case: 423\n",
      "22.07 % Case finished: 1801 Running case: 467\n",
      "22.83 % Case finished: 1855 Running case: 473\n",
      "23.59 % Case finished: 1867 Running case: 499\n",
      "24.35 % Case finished: 1910 Running case: 527\n",
      "25.11 % Case finished: 2047 Running case: 423\n",
      "25.87 % Case finished: 2133 Running case: 373\n",
      "26.63 % Case finished: 2207 Running case: 368\n",
      "27.39 % Case finished: 2276 Running case: 377\n",
      "28.15 % Case finished: 2366 Running case: 364\n",
      "28.91 % Case finished: 2434 Running case: 368\n",
      "29.68 % Case finished: 2504 Running case: 370\n",
      "30.44 % Case finished: 2594 Running case: 337\n",
      "31.2 % Case finished: 2630 Running case: 371\n",
      "31.96 % Case finished: 2699 Running case: 360\n",
      "32.72 % Case finished: 2795 Running case: 329\n",
      "33.48 % Case finished: 2899 Running case: 303\n",
      "34.24 % Case finished: 2970 Running case: 321\n",
      "35.0 % Case finished: 3030 Running case: 312\n",
      "35.76 % Case finished: 3118 Running case: 284\n",
      "36.52 % Case finished: 3239 Running case: 282\n",
      "37.28 % Case finished: 3316 Running case: 332\n",
      "38.05 % Case finished: 3400 Running case: 378\n",
      "38.81 % Case finished: 3477 Running case: 416\n"
     ]
    }
   ],
   "source": [
    "case_dict ={}\n",
    "for x,y in dataset:\n",
    "    if rowcounter%500 == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running case: %s'%(running_case))\n",
    "    rowcounter +=1\n",
    "    # Event stream change dictionary keys\n",
    "    x = utils.dictkey_chg(x, key_pair)\n",
    "#     x['ts'] = x['ts'][:-4]\n",
    "    # Check label possible\n",
    "    # x = utils.set_label(x)\n",
    "    x['outcome'] =y \n",
    "    # Initialize case by prefix length\n",
    "    caseid = x['caseid']\n",
    "    outcome = x['outcome']\n",
    "    x.pop('caseid')\n",
    "    x.pop('outcome')\n",
    "    \n",
    "    if caseid not in usedingrace:\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "\n",
    "        if caseid not in list(case_dict.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            case_dict[caseid] = []\n",
    "            running_case +=1\n",
    "        elif caseid in finishedcases:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(case_dict[caseid])+1)\n",
    "            case_bin.set_prev_enc(case_dict[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded()\n",
    "        ts = case_bin.event['ts']\n",
    "        if case_bin.prefix_length >=2 and case_bin.prefix_length <= maximum_prefix:\n",
    "            case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "            x_test = case_bin.encoded\n",
    "            model = training_models['prefix_%s'%(case_bin.prefix_length)][0]\n",
    "            y_pred = model.predict_one(x_test)\n",
    "            if type(y_pred) != str:\n",
    "                y_pred = 'False'\n",
    "            modelid,pred_value = copy.deepcopy(training_models['prefix_%s'%(case_bin.prefix_length)][2]), copy.deepcopy(y_pred)\n",
    "            case_bin.update_prediction((modelid, (pred_value,ts)))        \n",
    "        case_dict[caseid].append(case_bin)\n",
    "\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finishedcases:\n",
    "            finishedcases.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1    \n",
    "            # Real training start\n",
    "\n",
    "            # Modify encoded attributes of cases with feature matrix\n",
    "            case_length = len(case_dict[caseid])\n",
    "            if case_length >maximum_prefix:\n",
    "                case_length =maximum_prefix\n",
    "            y = outcome\n",
    "            for prefix in range(1, case_length):\n",
    "                case_dict[caseid][prefix].update_truelabel(y)\n",
    "                x = case_dict[caseid][prefix].encoded\n",
    "                model = training_models['prefix_%s'%(prefix+1)][0]\n",
    "                model.learn_one(x,y)\n",
    "                training_models['prefix_%s'%(prefix+1)][2] +=1\n",
    "                y_pred = model.predict_one(x)\n",
    "                training_models['prefix_%s'%(prefix+1)][1].update(y,y_pred)\n",
    "\n",
    "                for cases in list(case_dict.keys()):\n",
    "                    if len(case_dict[cases]) >prefix:\n",
    "                        x_test = case_dict[cases][prefix].encoded\n",
    "                        y_pred = model.predict_one(x_test)\n",
    "                        modelid,pred_value = copy.deepcopy(training_models['prefix_%s'%(prefix+1)][2]), copy.deepcopy(y_pred)\n",
    "                        case_dict[cases][prefix].update_prediction((modelid, (pred_value,ts)))\n",
    "                        prediction_key = str(cases)+'_'+str(prefix+1)\n",
    "                        if str(cases)+'_'+str(prefix+2) not in prediction_result.keys():\n",
    "                            if prediction_key not in prediction_result.keys():\n",
    "                                prediction_result[prediction_key] = {}\n",
    "                                prediction_result[prediction_key][modelid] = (pred_value,ts)\n",
    "                            else:\n",
    "                                prediction_result[prediction_key][modelid] = (pred_value,ts)\n",
    "#             print(case_dict[caseid][-2].predicted)\n",
    "            copying = copy.deepcopy(case_dict[caseid])\n",
    "            resultdict[caseid] = copying\n",
    "            case_dict[caseid] =[]\n",
    "            running_case -=1\n",
    "\n",
    "            for prefix in training_models.keys():\n",
    "                if prefix not in list(acc_dict.keys()):\n",
    "                    acc_dict[prefix]=[training_models[prefix][1].get()]\n",
    "                else:\n",
    "                    acc_dict[prefix].append(training_models[prefix][1].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "for t in list(acc_dict.keys())[:10]:\n",
    "    plt.plot(acc_dict[t], label=str(t))\n",
    "    plt.legend(ncol=2,loc='lower right')\n",
    "plt.title('EFDT accuracy update case by prefix length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Finished cases')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./img/efdt_acc_update.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "plt.title('Accuracy by prefix length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Prefix length')\n",
    "plt.xticks(rotation='45')\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.5, 1.0])\n",
    "\n",
    "plt.plot(list(acc_dict.keys()),[acc_dict[x][-1] for x in acc_dict.keys()], '-o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in training_models.keys():\n",
    "    print(m, training_models[m][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "for t in range(2,11):\n",
    "    testcases =[]\n",
    "    testpred = []\n",
    "    testtrue = []\n",
    "\n",
    "    print('prefix_%s'%(t))\n",
    "    for cases in (np.random.choice(list(resultdict.keys()), 1500)):\n",
    "        if len(resultdict[cases])>t:\n",
    "            y_pred = training_models['prefix_%s'%(t+1)][0].predict_one(resultdict[cases][t].encoded)\n",
    "            testpred.append(y_pred)\n",
    "            testtrue.append(resultdict[cases][t].true_label)\n",
    "    metric = metrics.Accuracy()\n",
    "    cm = metrics.ConfusionMatrix()\n",
    "    for yt,yp in zip(testtrue,testpred):\n",
    "        metric = metric.update(yt,yp)\n",
    "        cm = cm.update(yt,yp)\n",
    "    print(metric)\n",
    "    print(cm)\n",
    "    print(classification_report(testtrue,testpred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def averaged_prediction(bin_result_list):\n",
    "    return sorted(Counter(bin_result_list).items(),key = (lambda x:x[1]),reverse=True)[0][0]\n",
    "\n",
    "def get_bin_list(current_event_ts, next_event_ts, bin_n):\n",
    "    binsize = (next_event_ts -current_event_ts).total_seconds()/bin_n\n",
    "    binlist= []\n",
    "    prev_bin=current_event_ts\n",
    "    for t in range(bin_n):\n",
    "        binlist.append((prev_bin, prev_bin +datetime.timedelta(seconds=binsize)))\n",
    "        prev_bin = prev_bin +datetime.timedelta(seconds=binsize)\n",
    "\n",
    "    return binlist\n",
    "\n",
    "def averaged_prediction_by_bin(bin_list, event_prediction):\n",
    "    bin_result_dict = {}\n",
    "    for pos, t in enumerate(bin_list):\n",
    "        t = (t[0],t[1],pos+1)\n",
    "        bin_result_dict[t] = []\n",
    "    prev_bin_t = list(bin_result_dict.keys())[0]\n",
    "    for bin_t in bin_result_dict.keys():\n",
    "        for result in event_prediction.values():\n",
    "            if result[1] >=  bin_t[0] and result[1] <  bin_t[1]:\n",
    "                bin_result_dict[bin_t].append(result[0])\n",
    "            elif bin_t[0]==bin_t[1] and result[1] == bin_t[0]:\n",
    "                bin_result_dict[bin_t].append(result[0])                       \n",
    "        if len(bin_result_dict[bin_t]) ==0:\n",
    "            bin_result_dict[bin_t].append(bin_result_dict[prev_bin_t])\n",
    "        prev_bin_t = bin_t\n",
    "        bin_result_dict[bin_t] = averaged_prediction(bin_result_dict[bin_t])\n",
    "    return bin_result_dict\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "def continuous_evaluation(resultdict, bin_n):\n",
    "    bin_pred = {}\n",
    "    y_true = {}\n",
    "    counting = 0\n",
    "    for case in tqdm(resultdict.keys()):\n",
    "        if len(resultdict[case]) > 2:\n",
    "            for event in range(1,len(resultdict[case])-1):\n",
    "                current_event_ts = resultdict[case][event].event['ts']\n",
    "                next_event_ts = resultdict[case][event+1].event['ts']\n",
    "                bin_list = get_bin_list(current_event_ts, next_event_ts, bin_n)\n",
    "                try:\n",
    "                    t = averaged_prediction_by_bin(bin_list,resultdict[case][event].predicted)\n",
    "                    y_true[str(case)+'_'+str(event+1)]=resultdict[case][event].true_label\n",
    "                    bin_pred[str(case)+'_'+str(event+1)]=list(t.values())\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return y_true, bin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_n = 50\n",
    "\n",
    "bin_event_acc = {}\n",
    "y_true, target_pred = continuous_evaluation(resultdict, bin_n)\n",
    "bin_event_acc = {}\n",
    "\n",
    "for t in tqdm(range(2,12)):\n",
    "    bin_y_true = []\n",
    "    bin_y_pred = []\n",
    "    if 'prefix_%s'%(t) not in list(bin_event_acc.keys()):\n",
    "        bin_event_acc['prefix_%s'%(t)]=[]\n",
    "    for prefix in y_true.keys():\n",
    "        if t ==int(prefix.split('_')[1]):\n",
    "            bin_y_true.append(y_true[prefix])\n",
    "            bin_y_pred.append(target_pred[prefix])\n",
    "#     print(bin_y_pred)\n",
    "    for bin_interval in range(bin_n):\n",
    "        bin_y_pred2 = [x[bin_interval] for x in bin_y_pred]\n",
    "        bin_event_acc['prefix_%s'%(t)].append(accuracy_score(bin_y_true, bin_y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "\n",
    "for t in bin_event_acc.keys():\n",
    "    plt.plot(bin_event_acc[t],'-o',label=t)\n",
    "    plt.legend(ncol=2,loc='lower right')\n",
    "plt.title('EFDT Accuracy of continuous evaluation by prefix length and bin updates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Bin updates')\n",
    "plt.axis([-2, 52, 0.5, 1.0])\n",
    "plt.tight_layout()\n",
    "plt.savefig('./img/EFDT continuous acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "x = list(acc_dict.keys())\n",
    "y = [acc_dict[x][-1] for x in acc_dict.keys()]\n",
    "with open('./result/efdt_lastacc.pkl','wb') as f:\n",
    "    pkl.dump([x,y],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
