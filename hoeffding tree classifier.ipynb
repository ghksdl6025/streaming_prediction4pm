{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream,tree,metrics\n",
    "import utils\n",
    "from encoding import prefix_bin\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stream.iter_csv(\n",
    "            './data/bac_online_small.csv',\n",
    "            )\n",
    "\n",
    "totallength = len(list(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stream.iter_csv(\n",
    "            './data/bac_online_small.csv',\n",
    "            drop=['END_DATE','ROLE','CLOSURE_TYPE','CLOSURE_REASON','WORKING_STATE','case_cost'],\n",
    "            target='outcome'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Event stream entry**  \n",
    "----------  \n",
    "**Example 1)**  \n",
    "({'REQUEST_ID': '20175000168',  \n",
    "  'ACTIVITY': 'Service closure Request with network responsibility',  \n",
    "  'START_DATE': '2018-10-10 12:48:12.000',  \n",
    "  'CE_UO': '1'},  \n",
    " '')  \n",
    "   \n",
    "**Example 2)**  \n",
    "({'REQUEST_ID': '20175000168',  \n",
    "  'ACTIVITY': 'Request completed with account closure',  \n",
    "  'START_DATE': '2018-10-17 03:03:11.000',  \n",
    "  'CE_UO': 'BOF'},  \n",
    " 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pair = {\n",
    "        'REQUEST_ID':'caseid',\n",
    "        'ACTIVITY':'activity',\n",
    "        'START_DATE':'ts',\n",
    "        'CE_UO':'resource'\n",
    "}\n",
    "\n",
    "case_dict ={}\n",
    "training_models ={}\n",
    "feature_matrix ={}\n",
    "casecount = 0\n",
    "rowcounter = 0\n",
    "resultdict ={}\n",
    "acc_dict ={}\n",
    "running_case = 0\n",
    "prediction_result = {}\n",
    "graceperiod_finish=0\n",
    "finishedcases = set()\n",
    "usedingrace = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % Case finished: 0\n",
      "0.76 % Case finished: 56\n",
      "1.52 % Case finished: 126\n",
      "2.28 % Case finished: 191\n"
     ]
    }
   ],
   "source": [
    "for x,y in dataset:\n",
    "    if rowcounter%500 == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount))\n",
    "    rowcounter +=1\n",
    "    # Event stream change dictionary keys\n",
    "    x = utils.dictkey_chg(x, key_pair)\n",
    "#     x['ts'] = x['ts'][:-4]\n",
    "    # Check label possible\n",
    "    # x = utils.set_label(x)\n",
    "    x['outcome'] =y \n",
    "    # Initialize case by prefix length\n",
    "    caseid = x['caseid']\n",
    "    outcome = x['outcome']\n",
    "#     progress = x['progress']\n",
    "\n",
    "    x.pop('caseid')\n",
    "    x.pop('outcome')\n",
    "    \n",
    "#     x.pop('progress')\n",
    "\n",
    "    case_bin = prefix_bin(caseid, x)\n",
    "\n",
    "    if caseid not in list(case_dict.keys()):\n",
    "        case_bin.set_prefix_length(1)    \n",
    "        case_dict[caseid] = []\n",
    "    elif caseid in finishedcases:\n",
    "        pass\n",
    "    else:\n",
    "        case_bin.set_prefix_length(len(case_dict[caseid])+1)\n",
    "        case_bin.set_prev_enc(case_dict[caseid][-1])\n",
    "    \n",
    "    # Encode event and cases and add to DB\n",
    "    case_bin.update_truelabel(outcome)   \n",
    "    case_bin.update_encoded()\n",
    "    ts = case_bin.event['ts']\n",
    "    case_dict[caseid].append(case_bin)\n",
    "    usedingrace.add(caseid)\n",
    "    # Detect label appeared case \n",
    "    if outcome != '' and caseid not in finishedcases:\n",
    "        finishedcases.add(caseid)\n",
    "        # Adding newly finished case to training set.    \n",
    "        casecount +=1\n",
    "        # Grace period to collect feature matrix\n",
    "        if casecount <200:\n",
    "            case_length = len(case_dict[caseid])\n",
    "            for prefix in range(1, case_length):\n",
    "                if 'prefix_%s'%(prefix+1) not in list(feature_matrix.keys()):\n",
    "                    feature_matrix['prefix_%s'%(prefix+1)]=set()\n",
    "                    # Initialize classifier and performance matrix and updating count\n",
    "                    training_models['prefix_%s'%(prefix+1)] = [tree.HoeffdingTreeClassifier(grace_period=100,split_criterion='info_gain'),metrics.Accuracy(), 0,0]\n",
    "                feature_list = list(case_dict[caseid][prefix].encoded.keys())\n",
    "                for x in feature_list: feature_matrix['prefix_%s'%(prefix+1)].add(x) \n",
    "            graceperiod_finish = case_dict[caseid][-1].event['ts']\n",
    "            for t in training_models.keys():\n",
    "                training_models[t][3] = graceperiod_finish\n",
    "            case_dict.pop(caseid)            \n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_prefix = sorted([int(x.split('_')[1]) for x in training_models.keys()])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57 % Case finished: 326 Running case: 250\n"
     ]
    }
   ],
   "source": [
    "case_dict ={}\n",
    "for x,y in dataset:\n",
    "    if rowcounter%3000 == 0:\n",
    "        print(round(rowcounter*100/totallength,2) ,'%', 'Case finished: %s'%(casecount), 'Running case: %s'%(running_case))\n",
    "    rowcounter +=1\n",
    "    # Event stream change dictionary keys\n",
    "    x = utils.dictkey_chg(x, key_pair)\n",
    "#     x['ts'] = x['ts'][:-4]\n",
    "    # Check label possible\n",
    "    # x = utils.set_label(x)\n",
    "    x['outcome'] =y \n",
    "    # Initialize case by prefix length\n",
    "    caseid = x['caseid']\n",
    "    outcome = x['outcome']\n",
    "    x.pop('caseid')\n",
    "    x.pop('outcome')\n",
    "    \n",
    "    if caseid not in usedingrace:\n",
    "        case_bin = prefix_bin(caseid, x)\n",
    "\n",
    "        if caseid not in list(case_dict.keys()):\n",
    "            case_bin.set_prefix_length(1)    \n",
    "            case_dict[caseid] = []\n",
    "            running_case +=1\n",
    "        elif caseid in finishedcases:\n",
    "            pass\n",
    "        else:\n",
    "            case_bin.set_prefix_length(len(case_dict[caseid])+1)\n",
    "            case_bin.set_prev_enc(case_dict[caseid][-1])\n",
    "\n",
    "        # Encode event and cases and add to DB\n",
    "        case_bin.update_truelabel(outcome)   \n",
    "        case_bin.update_encoded()\n",
    "        ts = case_bin.event['ts']\n",
    "        if case_bin.prefix_length >=2 and case_bin.prefix_length <= maximum_prefix:\n",
    "            case_bin.encoded = utils.readjustment_training(case_bin.encoded, feature_matrix['prefix_%s'%(case_bin.prefix_length)])\n",
    "            x_test = case_bin.encoded\n",
    "            model = training_models['prefix_%s'%(case_bin.prefix_length)][0]\n",
    "            y_pred = model.predict_one(x_test)\n",
    "            if type(y_pred) != str:\n",
    "                y_pred = 'False'\n",
    "            modelid,pred_value = copy.deepcopy(training_models['prefix_%s'%(case_bin.prefix_length)][2]), copy.deepcopy(y_pred)\n",
    "            case_bin.update_prediction((modelid, (pred_value,ts)))        \n",
    "        case_dict[caseid].append(case_bin)\n",
    "\n",
    "        # Detect label appeared case \n",
    "        if outcome != '' and caseid not in finishedcases:\n",
    "            finishedcases.add(caseid)\n",
    "            # Adding newly finished case to training set.    \n",
    "            casecount +=1    \n",
    "            # Real training start\n",
    "\n",
    "            # Modify encoded attributes of cases with feature matrix\n",
    "            case_length = len(case_dict[caseid])\n",
    "            if case_length >maximum_prefix:\n",
    "                case_length =maximum_prefix\n",
    "            y = outcome\n",
    "            for prefix in range(1, case_length):\n",
    "                case_dict[caseid][prefix].update_truelabel(y)\n",
    "                x = case_dict[caseid][prefix].encoded\n",
    "                model = training_models['prefix_%s'%(prefix+1)][0]\n",
    "                model.learn_one(x,y)\n",
    "                training_models['prefix_%s'%(prefix+1)][2] +=1\n",
    "                y_pred = model.predict_one(x)\n",
    "                training_models['prefix_%s'%(prefix+1)][1].update(y,y_pred)\n",
    "\n",
    "                for cases in list(case_dict.keys()):\n",
    "                    if len(case_dict[cases]) >prefix:\n",
    "                        x_test = case_dict[cases][prefix].encoded\n",
    "                        y_pred = model.predict_one(x_test)\n",
    "                        modelid,pred_value = copy.deepcopy(training_models['prefix_%s'%(prefix+1)][2]), copy.deepcopy(y_pred)\n",
    "                        case_dict[cases][prefix].update_prediction((modelid, (pred_value,ts)))\n",
    "                        prediction_key = str(cases)+'_'+str(prefix+1)\n",
    "                        if str(cases)+'_'+str(prefix+2) not in prediction_result.keys():\n",
    "                            if prediction_key not in prediction_result.keys():\n",
    "                                prediction_result[prediction_key] = {}\n",
    "                                prediction_result[prediction_key][modelid] = (pred_value,ts)\n",
    "                            else:\n",
    "                                prediction_result[prediction_key][modelid] = (pred_value,ts)\n",
    "#             print(case_dict[caseid][-2].predicted)\n",
    "            copying = copy.deepcopy(case_dict[caseid])\n",
    "            resultdict[caseid] = copying\n",
    "            case_dict[caseid] =[]\n",
    "            running_case -=1\n",
    "\n",
    "            for prefix in training_models.keys():\n",
    "                if prefix not in list(acc_dict.keys()):\n",
    "                    acc_dict[prefix]=[training_models[prefix][1].get()]\n",
    "                else:\n",
    "                    acc_dict[prefix].append(training_models[prefix][1].get())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "for t in acc_dict.keys():\n",
    "    plt.plot(acc_dict[t], label=str(t))\n",
    "    plt.legend(ncol=2,loc='lower right')\n",
    "plt.title('HTC accuracy update case by prefix length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Finished cases')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./img/htc_acc_update.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous evaluation performance by relative time-event bin \n",
    "\n",
    "bin_n = 50\n",
    "rt_bin_event_acc = {}\n",
    "y_true, target_pred = utils.rt_event_continuous_evaluation(resultdict, bin_n)\n",
    "\n",
    "for t in tqdm(range(2,12)):\n",
    "    bin_y_true = []\n",
    "    bin_y_pred = []\n",
    "    if 'prefix_%s'%(t) not in list(bin_event_acc.keys()):\n",
    "        bin_event_acc['prefix_%s'%(t)]=[]\n",
    "    for prefix in y_true.keys():\n",
    "        if t ==int(prefix.split('_')[1]):\n",
    "            bin_y_true.append(y_true[prefix])\n",
    "            bin_y_pred.append(target_pred[prefix])\n",
    "#     print(bin_y_pred)\n",
    "    for bin_interval in range(bin_n):\n",
    "        bin_y_pred2 = [x[bin_interval] for x in bin_y_pred]\n",
    "        bin_event_acc['prefix_%s'%(t)].append(accuracy_score(bin_y_true, bin_y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def averaged_prediction(bin_result_list):\n",
    "    '''\n",
    "    Get single prediction value by bin result list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bin_result_list: list\n",
    "        List of predicted values of the bin\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    The most frequent singe value in arg max value from bin_result_list\n",
    "    '''\n",
    "    try:\n",
    "        answer =sorted(Counter(bin_result_list).items(),key = (lambda x:x[1]),reverse=True)[0][0] \n",
    "    except:\n",
    "        answer = None\n",
    "    return answer\n",
    "\n",
    "def pl_case_continuous_evaluation(resultdict):\n",
    "    bin_pred = {}\n",
    "    y_true = {}\n",
    "    for case in tqdm(resultdict.keys()):\n",
    "        if len(resultdict[case]) > 2:\n",
    "            for event in range(1,len(resultdict[case])-1):\n",
    "                bin_result_list = [x[0] for x in resultdict[case][event].predicted.values()]\n",
    "                bin_predicted_value = averaged_prediction(bin_result_list)\n",
    "                if bin_predicted_value != None:\n",
    "                    y_true[str(case)+'_'+str(event+1)] = resultdict[case][event].true_label\n",
    "                    bin_pred[str(case)+'_'+str(event+1)] = bin_predicted_value\n",
    "    return y_true, bin_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous evaluation performance by prefix length-case bin \n",
    "\n",
    "y_true, target_pred = pl_case_continuous_evaluation(resultdict)\n",
    "bin_event_acc = {}\n",
    "max_length = max([len(x) for x in resultdict])\n",
    "print(max_length)\n",
    "\n",
    "for prefix in tqdm(list(y_true.keys())):\n",
    "    t = int(prefix.split('_')[1])\n",
    "    if 'prefix_%s'%(t) not in list(bin_event_acc.keys()):\n",
    "        bin_event_acc['prefix_%s'%(t)] ={'y_true':[],'y_pred':[]}\n",
    "    else:\n",
    "        bin_event_acc['prefix_%s'%(t)]['y_true'].append(y_true[prefix])\n",
    "        bin_event_acc['prefix_%s'%(t)]['y_pred'].append(target_pred[prefix])\n",
    "\n",
    "for prefix in bin_event_acc.keys():\n",
    "    bin_event_acc[prefix]= accuracy_score(bin_event_acc[prefix]['y_true'], bin_event_acc[prefix]['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bin_event_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_case_continuous_evaluation(resultdict, bin_n):\n",
    "    bin_pred = {}\n",
    "    y_true = {}\n",
    "    for case in tqdm(resultdict.keys()):\n",
    "        if len(resultdict[case]) > 2:\n",
    "            prediction_counter = 0\n",
    "            prediction_by_bin ={}\n",
    "            start_ts = resultdict[case][1].event['ts']\n",
    "            end_ts = resultdict[case][-1].event['ts']\n",
    "            bin_list = utils.get_ts_bin_list(start_ts, end_ts, bin_n)\n",
    "            for event in range(1,len(resultdict[case])-1):\n",
    "                for predict in resultdict[case][event].predicted.keys():\n",
    "                    prediction_by_bin[prediction_counter] = resultdict[case][event].predicted[predict]\n",
    "                    prediction_counter +=1\n",
    "            t = utils.ts_averaged_prediction_by_bin(bin_list,prediction_by_bin)\n",
    "            for each_bin in [x for x in t.keys()]:                \n",
    "                y_true[str(case)+'_'+str(each_bin[2]+1)]=resultdict[case][event].true_label\n",
    "                bin_pred[str(case)+'_'+str(each_bin[2]+1)]= t[each_bin]\n",
    "\n",
    "#                 y_true[str(case)+'_'+str(event+1)] = resultdict[case][event].true_label\n",
    "#                 bin_pred[str(case)+'_'+str(event+1)] = averaged_prediction(bin_result_list)\n",
    "            \n",
    "    return y_true, bin_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous evaluation performance by relative time-case bin \n",
    "\n",
    "bin_n = 100\n",
    "bin_event_acc = {}\n",
    "y_true, target_pred = rt_case_continuous_evaluation(resultdict, bin_n)\n",
    "\n",
    "# for t in tqdm(range(1,51)):\n",
    "#     bin_y_true = []\n",
    "#     bin_y_pred = []\n",
    "#     if 'Bin_%s'%(t) not in list(bin_event_acc.keys()):\n",
    "#         bin_event_acc['Bin_%s'%(t)]=0\n",
    "        \n",
    "#     for prefix in y_true.keys():\n",
    "#         if t ==int(prefix.split('_')[1]):\n",
    "#             bin_y_true.append(y_true[prefix])\n",
    "#             bin_y_pred.append(target_pred[prefix])\n",
    "#             bin_event_acc['Bin_%s'%(t)]= accuracy_score(bin_y_true, bin_y_pred)\n",
    "            \n",
    "for prefix in tqdm(list(y_true.keys())):\n",
    "    t = int(prefix.split('_')[1])\n",
    "    if 'Bin_%s'%(t) not in list(bin_event_acc.keys()):\n",
    "        bin_event_acc['Bin_%s'%(t)] ={'y_true':[],'y_pred':[]}\n",
    "    else:\n",
    "        bin_event_acc['Bin_%s'%(t)]['y_true'].append(y_true[prefix])\n",
    "        bin_event_acc['Bin_%s'%(t)]['y_pred'].append(target_pred[prefix])\n",
    "\n",
    "for prefix in bin_event_acc.keys():\n",
    "    bin_event_acc[prefix]= accuracy_score(bin_event_acc[prefix]['y_true'], bin_event_acc[prefix]['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([-0.05, 1.0])\n",
    "x,ce_acc_y = list(bin_event_acc.keys()),[bin_event_acc[x] for x in bin_event_acc.keys()]\n",
    "last_acc_y = [acc_dict[x][-1] for x in acc_dict.keys()]\n",
    "plt.plot(x,ce_acc_y,'-o',label='Continuous evaluation')\n",
    "plt.plot(x,last_acc_y, '-o',label = 'Last accuracy')\n",
    "\n",
    "y_gap = [[t, last_acc_y[pos]] if t<= last_acc_y[pos] else (last_acc_y[pos], t) for pos,t in enumerate(ce_acc_y) ]\n",
    "for pos in range(len(y_gap)):\n",
    "    if y_gap[pos][0] ==0:\n",
    "        y_gap[pos][0] = 0.05\n",
    "    if y_gap[pos][1] ==0:\n",
    "        y_gap[pos][1] = 0.05\n",
    "plt.title('Continuous evaluation accuracy by prefix length bin')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Prefix length')\n",
    "plt.xticks(rotation='45')\n",
    "\n",
    "for gap in range(len(y_gap)):\n",
    "    print(y_gap[gap])\n",
    "    plt.axvline(x=gap, ymin= y_gap[gap][0], ymax = y_gap[gap][1],color='b',ls=':', label='axvline - full height')\n",
    "    \n",
    "\n",
    "# plt.savefig('./img/HTC continuous acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.rc('axes', titlesize=18) # fontsize of the axes title \n",
    "plt.rc('axes', labelsize=18) # fontsize of the x and y labels \n",
    "plt.rc('xtick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('ytick', labelsize=15) # fontsize of the tick labels \n",
    "plt.rc('legend', fontsize=14) # legend fontsize \n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.92, 0.94])\n",
    "x,ce_acc_y = list(bin_event_acc.keys()),[bin_event_acc[x] for x in bin_event_acc.keys()]\n",
    "plt.plot(x,ce_acc_y,'-o',label='Continuous evaluation')\n",
    "# plt.plot(x,last_acc_y, '-o',label = 'Last accuracy')\n",
    "\n",
    "plt.title('Continuous evaluation accuracy by prefix length bin')\n",
    "plt.legend()\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Prefix length')\n",
    "plt.xticks(rotation='45')\n",
    "    \n",
    "plt.show()\n",
    "# plt.savefig('./img/HTC continuous acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "uio = []\n",
    "for t in acc_dict.keys():\n",
    "    uio.append(t)\n",
    "y = [acc_dict[x][-1] for x in acc_dict.keys()]\n",
    "with open('./result/htc_lastacc.pkl','wb') as f:\n",
    "    pkl.dump([uio,y],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
