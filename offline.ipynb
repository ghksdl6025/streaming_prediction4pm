{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import utils\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_prefix(df,prefix):\n",
    "    '''\n",
    "    Filter case by prefix length\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to slice by prefix length\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe with sliced cases\n",
    "    '''\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    for case,group in groups: \n",
    "        group = group.reset_index(drop=True)\n",
    "        if len(group)>prefix:\n",
    "            group = group.loc[:prefix-1,:]\n",
    "            encoded_df.append(group)\n",
    "    return pd.concat(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_encoding(df, prefix):\n",
    "    '''\n",
    "    Aggregation encoding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to encode for outcome prediction\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe encoded in aggregation method\n",
    "    '''\n",
    "    df = filter_by_prefix(df,prefix)\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    for case,group in groups: \n",
    "        group = group.reset_index(drop=True)\n",
    "        outcome = set(group['outcome']).pop()\n",
    "        cumdurationlist = [(x - list(group['ts'])[0]).total_seconds() for x in list(group['ts'])]\n",
    "        case_time_outcome = {'caseid':case, 'ts':np.mean(cumdurationlist),'outcome':outcome}\n",
    "        activity_count = {x: list(group['activity']).count(x) for x in set(group['activity'])}\n",
    "        resource_count = {x: list(group['resource']).count(x) for x in set(group['resource'])}\n",
    "\n",
    "        case_time_outcome.update(activity_count)\n",
    "        case_time_outcome.update(resource_count)\n",
    "        dfk = pd.DataFrame.from_dict([case_time_outcome])\n",
    "        encoded_df.append(dfk)\n",
    "    concated_df = pd.concat(encoded_df)\n",
    "    concated_df = concated_df.fillna(0)\n",
    "    return concated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexbase_encoding(df, prefix):\n",
    "    '''\n",
    "    Indexbase encoding\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        Assigned dataframe to encode for outcome prediction\n",
    "    \n",
    "    prefix : int\n",
    "        Prefix length to slice to cases in fixed length\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Return dataframe encoded in indexbase method\n",
    "    '''\n",
    "    df = filter_by_prefix(df,prefix)\n",
    "    df['ts'] = pd.to_datetime(df['ts'])\n",
    "    groups = df.groupby('caseid')\n",
    "    encoded_df=[]\n",
    "    if 'resource' not in list(df.columns.values):\n",
    "        noresource = True\n",
    "    else:\n",
    "        noresource = False\n",
    "        \n",
    "    for case,group in groups: \n",
    "        activitylist = list(group['activity'])\n",
    "        \n",
    "        group = group.reset_index(drop=True)\n",
    "        outcome = set(group['outcome']).pop()\n",
    "        cumdurationlist = [(x - list(group['ts'])[0]).total_seconds() for x in list(group['ts'])]\n",
    "        cumduration_index ={'Cumduration_'+str(x+1): cumdurationlist[x] for x in range(len(cumdurationlist))}\n",
    "        \n",
    "        case_outcome = {'caseid':case, 'outcome':outcome}\n",
    "        activity_index = {'activity_'+str(x+1)+'_'+activitylist[x]: 1 for x in range(len(activitylist))}\n",
    "\n",
    "        if noresource == False:\n",
    "            resourcelist = list(group['resource'])\n",
    "            resource_index = {'resource_'+str(x+1)+'_'+str(resourcelist[x]): 1 for x in range(len(resourcelist))}\n",
    "            case_outcome.update(resource_index)\n",
    "        \n",
    "        case_outcome.update(cumduration_index)\n",
    "        case_outcome.update(activity_index)\n",
    "        dfk = pd.DataFrame.from_dict([case_outcome])\n",
    "        encoded_df.append(dfk)\n",
    "    concated_df = pd.concat(encoded_df)\n",
    "    concated_df = concated_df.fillna(0)\n",
    "    return concated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label = 'bpic17'\n",
    "with open('./dataset_parameters.json','r') as json_file:\n",
    "    parameters = json.load(json_file)[dataset_label]\n",
    "    key_pair = parameters['key_pair']\n",
    "    maximum_prefix = parameters['maximum_prefix']\n",
    "\n",
    "dataset_loc = './data/' +dataset_label +'.csv'\n",
    "df = pd.read_csv(dataset_loc)\n",
    "df = df.rename(columns=key_pair)\n",
    "if 'resource' in df.columns.values:\n",
    "    df = df.loc[:,['caseid','activity','ts','resource','outcome']]\n",
    "else:\n",
    "    df = df.loc[:,['caseid','activity','ts','outcome']]\n",
    "\n",
    "try:\n",
    "    os.makedirs('./result/%s'%(dataset_label))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('caseid')\n",
    "concating = []\n",
    "for _, group in groups:\n",
    "    outcomelist = list(group['outcome'])\n",
    "    outcome = outcomelist[-1]\n",
    "    group = group.reset_index(drop=True)\n",
    "    if True in outcomelist:\n",
    "        group = group.loc[:outcomelist.index(True),:]\n",
    "    group['outcome'] = outcome\n",
    "    concating.append(group)\n",
    "\n",
    "dfn = pd.concat(concating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressing length: 2\n",
      "Progressing length: 3\n",
      "Progressing length: 4\n",
      "Progressing length: 5\n",
      "Progressing length: 6\n",
      "Progressing length: 7\n",
      "Progressing length: 8\n",
      "Progressing length: 9\n",
      "Progressing length: 10\n",
      "Progressing length: 11\n",
      "Progressing length: 12\n",
      "Progressing length: 13\n",
      "Progressing length: 14\n"
     ]
    }
   ],
   "source": [
    "idslist = []\n",
    "prefix_length=maximum_prefix\n",
    "for length in range(2,prefix_length):\n",
    "    print('Progressing length: %s'%(length))\n",
    "    idslist.append(indexbase_encoding(dfn,length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree\n",
      "{'prefix_2': 0.6890070921985816, 'prefix_3': 0.6132978723404255, 'prefix_4': 0.6387211367673178, 'prefix_5': 0.646797153024911, 'prefix_6': 0.632620320855615, 'prefix_7': 0.6417112299465241, 'prefix_8': 0.6501792114695341, 'prefix_9': 0.6363471971066909, 'prefix_10': 0.8155109489051096, 'prefix_11': 0.9706443914081146, 'prefix_12': 0.9537117903930131, 'prefix_13': 0.9578431372549019, 'prefix_14': 1.0}\n"
     ]
    }
   ],
   "source": [
    "prefixlist= list(range(2,prefix_length))\n",
    "acc_dict= {}\n",
    "print('Decision tree')\n",
    "\n",
    "for pos,prefix in enumerate(idslist):\n",
    "    y = prefix['outcome']\n",
    "    x =prefix.drop(columns=['outcome','caseid'],axis=1)\n",
    "    acc_list = []\n",
    "    for i in range(10):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3)\n",
    "\n",
    "        # Deicision tree result\n",
    "        dt = DecisionTreeClassifier(criterion='entropy').fit(x_train,y_train)\n",
    "        y_pred = dt.predict(x_test)\n",
    "        acc_list.append(accuracy_score(y_test,y_pred))\n",
    "    acc_dict['prefix_%s'%(str(prefixlist[pos]))] =  np.mean(acc_list)\n",
    "\n",
    "print(acc_dict)\n",
    "import pickle as pkl\n",
    "\n",
    "x = list(acc_dict.keys())\n",
    "y = [acc_dict[x] for x in acc_dict.keys()]\n",
    "with open('./result/%s/off_dt_acc.pkl'%(dataset_label),'wb') as f:\n",
    "    pkl.dump([x,y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest\n",
      "{'prefix_2': 0.6914893617021276, 'prefix_3': 0.6567375886524823, 'prefix_4': 0.6937833037300177, 'prefix_5': 0.7083629893238435, 'prefix_6': 0.7115864527629233, 'prefix_7': 0.7135472370766489, 'prefix_8': 0.7229390681003585, 'prefix_9': 0.7103074141048824, 'prefix_10': 0.8496350364963504, 'prefix_11': 0.9744630071599045, 'prefix_12': 0.9694323144104805, 'prefix_13': 0.969607843137255, 'prefix_14': 1.0}\n"
     ]
    }
   ],
   "source": [
    "prefixlist= list(range(2,prefix_length))\n",
    "acc_dict= {}\n",
    "print('Random forest')\n",
    "for pos,prefix in enumerate(idslist):\n",
    "    y = prefix['outcome']\n",
    "    x =prefix.drop(columns=['outcome','caseid'],axis=1)\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(10):\n",
    "        x_train,x_test,y_train,y_test = train_test_split(x,y, test_size=0.3)\n",
    "\n",
    "        # Random forest result\n",
    "        rf = RandomForestClassifier(criterion='entropy').fit(x_train,y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        acc_list.append(accuracy_score(y_test,y_pred))\n",
    "    acc_dict['prefix_%s'%(str(prefixlist[pos]))] =  np.mean(acc_list)\n",
    "print(acc_dict)\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "x = list(acc_dict.keys())\n",
    "y = [acc_dict[x] for x in acc_dict.keys()]\n",
    "with open('./result/%s/off_rf_acc.pkl'%(dataset_label),'wb') as f:\n",
    "    pkl.dump([x,y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
